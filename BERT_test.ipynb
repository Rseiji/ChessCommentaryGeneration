{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_test",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO58LVI3+85D8t2fwSAlsK4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rseiji/ChessCommentaryGeneration/blob/master/BERT_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-4YX_zhqoFL",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDz8LdICoC_S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "outputId": "145627b5-3b71-400e-ece9-c4f48f8361f4"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "!pip install transformers\n",
        "!pip install wget\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "from google.colab import drive"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/05/c8c55b600308dc04e95100dc8ad8a244dd800fe75dfafcf1d6348c6f6209/transformers-3.1.0-py3-none-any.whl (884kB)\n",
            "\r\u001b[K     |▍                               | 10kB 19.7MB/s eta 0:00:01\r\u001b[K     |▊                               | 20kB 6.5MB/s eta 0:00:01\r\u001b[K     |█▏                              | 30kB 6.9MB/s eta 0:00:01\r\u001b[K     |█▌                              | 40kB 7.9MB/s eta 0:00:01\r\u001b[K     |█▉                              | 51kB 6.8MB/s eta 0:00:01\r\u001b[K     |██▎                             | 61kB 7.3MB/s eta 0:00:01\r\u001b[K     |██▋                             | 71kB 8.3MB/s eta 0:00:01\r\u001b[K     |███                             | 81kB 8.8MB/s eta 0:00:01\r\u001b[K     |███▍                            | 92kB 8.2MB/s eta 0:00:01\r\u001b[K     |███▊                            | 102kB 8.9MB/s eta 0:00:01\r\u001b[K     |████                            | 112kB 8.9MB/s eta 0:00:01\r\u001b[K     |████▌                           | 122kB 8.9MB/s eta 0:00:01\r\u001b[K     |████▉                           | 133kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 143kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 153kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████                          | 163kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 174kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 184kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████                         | 194kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 204kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 215kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 225kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 235kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████                       | 245kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 256kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 266kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████                      | 276kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 286kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 296kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 307kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 317kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 327kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 337kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 348kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 358kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 368kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 378kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 389kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 399kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 409kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 419kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 430kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████                | 440kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 450kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 460kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 471kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 481kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 491kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 501kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 512kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 522kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 532kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 542kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 552kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 563kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 573kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 583kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 593kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 604kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 614kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 624kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 634kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 645kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 655kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 665kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 675kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 686kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 696kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 706kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 716kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 727kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 737kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 747kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 757kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 768kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 778kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 788kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 798kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 808kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 819kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 829kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 839kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 849kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 860kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 870kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 880kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 890kB 8.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 29.2MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 48.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 47.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=327c0329117838ec31200faa165201910f0f4d199b94d2ab1365b68a75cb708b\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.1.0\n",
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=fa736fb90ff005d1688baa2946d1731fd9ed518b1f44f364fdf9a9b582ec5007\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUYDsXnipj57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## GPU google colab\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynBZsmdEplRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lonxTq8mrIa4",
        "colab_type": "text"
      },
      "source": [
        "## Adding path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgweTu8ToDz_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0a3e0469-2b9e-4ecc-bdb2-f9bf52bf7848"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_5kw_w2rh2I",
        "colab_type": "text"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXXYVzSvJguL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reading input data\n",
        "# file_path = ..... inserir aqui o file path .....\n",
        "df = pd.read_csv(file_path)\n",
        "sentences = df['sentences'].tolist()\n",
        "labels = df['label'].tolist()\n",
        "\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6_NHH_dKBki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_phrases_maximun_len(sentences, default_max_len=None):\n",
        "  \"\"\"Bert demands a default maximun phrase len to work properly.\n",
        "  This lenght is measured by number of words. If the phrase is\n",
        "  smaller than maximun len, the remaining blank elements are filled\n",
        "  with padding tokens.\n",
        "  \"\"\"\n",
        "  if default_max_len:\n",
        "    return default_max_len\n",
        "\n",
        "  max_len = 0\n",
        "  # For every sentence...\n",
        "  for sent in sentences:\n",
        "      # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "      input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "      # Update the maximum sentence length.\n",
        "      max_len = max(max_len, len(input_ids))\n",
        "  print('Max sentence length: ', max_len)\n",
        "  return max_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lu1z_kZtmRM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_sentences(sentences, max_length):\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "\n",
        "  # For every sentence...\n",
        "  for sent in sentences:\n",
        "      # `encode_plus` will:\n",
        "      #   (1) Tokenize the sentence.\n",
        "      #   (2) Prepend the `[CLS]` token to the start.\n",
        "      #   (3) Append the `[SEP]` token to the end.\n",
        "      #   (4) Map tokens to their IDs.\n",
        "      #   (5) Pad or truncate the sentence to `max_length`\n",
        "      #   (6) Create attention masks for [PAD] tokens.\n",
        "      encoded_dict = tokenizer.encode_plus(\n",
        "                          sent,                      # Sentence to encode.\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                          max_length = 64,           # Pad & truncate all sentences.\n",
        "                          pad_to_max_length = True,\n",
        "                          return_attention_mask = True,   # Construct attn. masks.\n",
        "                          return_tensors = 'pt'     # Return pytorch tensors.\n",
        "                    )\n",
        "      \n",
        "      # Add the encoded sentence to the list.    \n",
        "      input_ids.append(encoded_dict['input_ids'])\n",
        "      \n",
        "      # And its attention mask (simply differentiates padding from non-padding).\n",
        "      attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "  # Convert the lists into tensors.\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "  return (input_ids, attention_masks)\n",
        "\n",
        "  # Print sentence 0, now as a list of IDs.\n",
        "  # print('Original: ', sentences[0])\n",
        "  # print('Token IDs:', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxxK6PrjPI1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_dataset(sentences, labels, max_phrase_len=None):\n",
        "  max_phrase_len = get_phrases_maximun_len(sentences, max_phrase_len)\n",
        "  input_ids, attention_masks = tokenize_sentences(sentences, max_phrase_len)\n",
        "  labels = torch.tensor(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Zv2AA7GuCgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_validation(input_ids, attention_masks, labels):\n",
        "  # Combine the training inputs into a TensorDataset.\n",
        "  dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "  # Create a 90-10 train-validation split.\n",
        "  # Calculate the number of samples to include in each set.\n",
        "  train_size = int(0.9 * len(dataset))\n",
        "  val_size = len(dataset) - train_size\n",
        "\n",
        "  # Divide the dataset by randomly selecting samples.\n",
        "  train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "  print('{:>5,} training samples'.format(train_size))\n",
        "  print('{:>5,} validation samples'.format(val_size))\n",
        "\n",
        "  return train_dataset, val_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chIveYFquITp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_loader(batch_size, train_dataset, val_dataset):\n",
        "  # The DataLoader needs to know our batch size for training, so we specify it \n",
        "  # here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "  # size of 16 or 32.\n",
        "  batch_size = 32\n",
        "\n",
        "  # Create the DataLoaders for our training and validation sets.\n",
        "  # We'll take training samples in random order. \n",
        "  train_dataloader = DataLoader(\n",
        "              train_dataset,  # The training samples.\n",
        "              sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "              batch_size = batch_size # Trains with this batch size.\n",
        "          )\n",
        "\n",
        "  # For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "  validation_dataloader = DataLoader(\n",
        "              val_dataset, # The validation samples.\n",
        "              sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "              batch_size = batch_size # Evaluate with this batch size.\n",
        "          )\n",
        "  return train_dataloader, validation_dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gjFm0-0uI7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPXS_Q7SqhW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ideia: \n",
        "# 1- Dataset: filtrar comentarios\n",
        "# 2- Engine: gerar labels\n",
        "# 3- BERT: setup\n",
        "# 4- BERT: definir interface (ultima camada)\n",
        "# 5- BERT: definir "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}